{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d854c90",
   "metadata": {},
   "source": [
    "# Clustering Assignment: Reproducible Experimental Pipeline\n",
    "\n",
    "This notebook implements an end-to-end clustering workflow designed for *comparative evaluation* across two dataset variants (**no augmentation** vs. **augmented**). The analysis emphasizes (i) transparent preprocessing, (ii) principled hyperparameter selection, and (iii) quantitative evaluation using internal validity (e.g., silhouette) and external agreement metrics (e.g., purity).\n",
    "\n",
    "**In the next cell, a minimal sanity check is executed to confirm that the notebook runs correctly before importing the full scientific stack.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Salam Alikom Habibi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f668e",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Runtime Utilities\n",
    "\n",
    "In this section, all required scientific-computing and machine-learning dependencies are imported (NumPy/Pandas/Matplotlib and scikit-learn). Optional notebook-only display helpers are also activated to improve readability when presenting intermediate tables and figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b223c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "\n",
    "# Optional: pretty side-by-side display inside notebooks\n",
    "try:\n",
    "    from IPython.display import display, HTML\n",
    "    IPY_OK = True\n",
    "except Exception:\n",
    "    IPY_OK = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219ebc3",
   "metadata": {},
   "source": [
    "## 2. Experimental Configuration and Reproducibility Controls\n",
    "\n",
    "Here, file-system paths (data and output directories), dataset *variants*, random seeds, and algorithm-specific hyperparameters are declared. Centralizing these settings supports reproducibility and facilitates systematic comparisons across clustering methods and preprocessing choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ac42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "RUN_TS = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = ROOT/\"outputs\"/ f\"outputs_{RUN_TS}\"\n",
    "EXPORT_DIR = RUN_DIR / \"csv_exports\"\n",
    "\n",
    "KINDS = [\"no_augmentation\", \"augmented\"]\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ---- Step1 (preprocess / optimized features) ----\n",
    "DROP_HIGH_CORR = True\n",
    "CORR_THR = 0.99\n",
    "\n",
    "USE_PCA = True\n",
    "PCA_VAR = 0.90          # variance ratio target (e.g. 0.90)\n",
    "PCA_WHITEN = False\n",
    "ROW_L2_NORM = False\n",
    "\n",
    "# ---- Step2 (KMeans) ----\n",
    "K_RANGE = list(range(2, 10))\n",
    "SIL_SAMPLE_SIZE = 1000\n",
    "\n",
    "# ---- Step3 (DBSCAN) ----\n",
    "DBSCAN_MIN_SAMPLES_LIST = [5, 10, 15, 20]\n",
    "DBSCAN_EPS_PERCENTILES = [60, 70, 80, 85, 90, 92, 95]\n",
    "TARGET_K = 4\n",
    "DBSCAN_MAX_NOISE_RATIO = 0.35\n",
    "\n",
    "# ---- Step4 (OPTICS) ----\n",
    "OPTICS_MIN_SAMPLES_LIST = [5, 10, 15, 20]\n",
    "OPTICS_XI_LIST = [0.03, 0.05, 0.08, 0.10]\n",
    "OPTICS_MIN_CLUSTER_SIZE_LIST = [0.03, 0.05, 0.08]\n",
    "OPTICS_MAX_NOISE_RATIO = 0.40\n",
    "\n",
    "# Create run folders\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[OK] RUN_DIR:\", RUN_DIR)\n",
    "print(\"[OK] DATA_DIR exists?\", DATA_DIR.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5bbe9",
   "metadata": {},
   "source": [
    "## 3. Utility Functions: I/O, Metrics, and Visualization Primitives\n",
    "\n",
    "This block defines reusable helper functions for: (a) robust directory and JSON/CSV I/O, (b) dataset loading from `*.npy` files, (c) evaluation metrics (including silhouette computations adapted for noise-aware clusterings), and (d) visualization utilities (e.g., PCA-based 2D projections for qualitative inspection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9bcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "def ensure_dirs(*paths: Path) -> None:\n",
    "    for p in paths:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_json(obj, path: Path) -> None:\n",
    "    ensure_dirs(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def side_by_side_tables(df_left: pd.DataFrame, df_right: pd.DataFrame,\n",
    "                        title_left: str = \"RAW\", title_right: str = \"OPT\") -> None:\n",
    "    if not IPY_OK:\n",
    "        print(title_left)\n",
    "        print(df_left)\n",
    "        print(\"\\n\" + title_right)\n",
    "        print(df_right)\n",
    "        return\n",
    "\n",
    "    html = f\"\"\"\n",
    "    <div style=\"display:flex; gap:24px; align-items:flex-start;\">\n",
    "      <div style=\"flex:1;\">\n",
    "        <h4 style=\"margin:0 0 8px 0;\">{title_left}</h4>\n",
    "        {df_left.to_html(index=False)}\n",
    "      </div>\n",
    "      <div style=\"flex:1;\">\n",
    "        <h4 style=\"margin:0 0 8px 0;\">{title_right}</h4>\n",
    "        {df_right.to_html(index=False)}\n",
    "      </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def load_feature_names(data_dir: Path) -> list[str]:\n",
    "    p = data_dir / \"feature_names.npy\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing feature_names.npy in: {data_dir}\")\n",
    "    arr = np.load(p, allow_pickle=True)\n",
    "    return [str(x) for x in arr]\n",
    "\n",
    "\n",
    "def load_dataset_npy(data_dir: Path, kind: str):\n",
    "    base = data_dir / kind\n",
    "    X_train = np.load(base / \"X_train.npy\")\n",
    "    X_test = np.load(base / \"X_test.npy\")\n",
    "    y_train = np.load(base / \"y_train.npy\", allow_pickle=True).astype(str)\n",
    "    y_test = np.load(base / \"y_test.npy\", allow_pickle=True).astype(str)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def prepare_compare_dirs(step_root: Path, kind: str):\n",
    "    raw_out = step_root / kind / \"raw\"\n",
    "    opt_out = step_root / kind / \"optimized\"\n",
    "    cmp_out = step_root / kind / \"compare\"\n",
    "    ensure_dirs(raw_out / \"tables\", raw_out / \"figures\", raw_out / \"meta\")\n",
    "    ensure_dirs(opt_out / \"tables\", opt_out / \"figures\", opt_out / \"meta\")\n",
    "    ensure_dirs(cmp_out / \"tables\", cmp_out / \"figures\", cmp_out / \"meta\")\n",
    "    return raw_out, opt_out, cmp_out\n",
    "\n",
    "\n",
    "def purity_score(cluster_labels: np.ndarray, true_labels: np.ndarray, noise_label: int | None = -1):\n",
    "    \"\"\"Purity (overall) + per-cluster table. If noise_label is not None, it will be ignored.\"\"\"\n",
    "    df = pd.DataFrame({\"cluster\": cluster_labels, \"true\": true_labels})\n",
    "\n",
    "    if noise_label is not None:\n",
    "        df = df[df[\"cluster\"] != noise_label]\n",
    "\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        empty = pd.DataFrame(columns=[\"cluster\", \"size\", \"top_label\", \"top_count\", \"purity_cluster\"])\n",
    "        return float(\"nan\"), empty\n",
    "\n",
    "    per_cluster = []\n",
    "    correct = 0\n",
    "    for c, g in df.groupby(\"cluster\"):\n",
    "        vc = g[\"true\"].value_counts()\n",
    "        top_label = vc.idxmax()\n",
    "        top_count = int(vc.max())\n",
    "        size = int(len(g))\n",
    "        correct += top_count\n",
    "        per_cluster.append(\n",
    "            {\n",
    "                \"cluster\": int(c),\n",
    "                \"size\": size,\n",
    "                \"top_label\": str(top_label),\n",
    "                \"top_count\": top_count,\n",
    "                \"purity_cluster\": float(top_count / size),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    overall = float(correct / total)\n",
    "    return overall, pd.DataFrame(per_cluster).sort_values(\"cluster\")\n",
    "\n",
    "\n",
    "def silhouette_excluding_noise(X: np.ndarray, labels: np.ndarray, noise_label: int = -1,\n",
    "                               sample_size: int | None = None) -> float:\n",
    "    mask = labels != noise_label\n",
    "    labs = labels[mask]\n",
    "    if mask.sum() < 10 or len(np.unique(labs)) < 2:\n",
    "        return float(\"nan\")\n",
    "    X_use = X[mask]\n",
    "    use_sample_size = sample_size if (sample_size is not None and X_use.shape[0] > sample_size) else None\n",
    "    return float(silhouette_score(X_use, labs, sample_size=use_sample_size, random_state=RANDOM_SEED))\n",
    "\n",
    "\n",
    "def choose_best_by_target_k(df_results: pd.DataFrame, target_k: int, max_noise_ratio: float) -> dict | None:\n",
    "    \"\"\"Choose best row: prefer n_clusters close to target_k, higher silhouette, lower noise.\"\"\"\n",
    "    df = df_results.copy()\n",
    "    df = df[df[\"n_clusters\"] >= 2]\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    if (df[\"noise_ratio\"] <= max_noise_ratio).any():\n",
    "        df = df[df[\"noise_ratio\"] <= max_noise_ratio]\n",
    "\n",
    "    df[\"k_distance\"] = (df[\"n_clusters\"] - target_k).abs()\n",
    "    df = df.sort_values([\"k_distance\", \"silhouette_excl_noise\", \"noise_ratio\"], ascending=[True, False, True])\n",
    "    best = df.iloc[0].to_dict()\n",
    "    return best\n",
    "\n",
    "\n",
    "def pca2_project(X: np.ndarray) -> np.ndarray:\n",
    "    return PCA(n_components=2, random_state=RANDOM_SEED).fit_transform(X)\n",
    "\n",
    "\n",
    "def encode_labels_for_cmap(labels: np.ndarray, include_noise: bool = True, noise_label: int = -1):\n",
    "    labels = np.asarray(labels)\n",
    "    is_str = labels.dtype.kind in [\"U\", \"S\", \"O\"]\n",
    "\n",
    "    if is_str:\n",
    "        uniq = sorted(pd.unique(labels).tolist())\n",
    "    else:\n",
    "        uniq = np.unique(labels)\n",
    "        uniq = np.sort(uniq)\n",
    "        if include_noise and (noise_label in uniq):\n",
    "            uniq = np.r_[noise_label, uniq[uniq != noise_label]]\n",
    "\n",
    "    mapping = {lab: i for i, lab in enumerate(uniq)}\n",
    "    enc = np.array([mapping[v] for v in labels])\n",
    "    return enc, uniq, mapping\n",
    "\n",
    "\n",
    "def plot_pca2_scatter(X: np.ndarray, labels: np.ndarray, title: str, outpath: Path,\n",
    "                      cmap_name: str = \"tab20\", include_noise: bool = True, noise_label: int = -1,\n",
    "                      legend_title: str = \"Labels\", show: bool = False) -> None:\n",
    "    X2 = pca2_project(X)\n",
    "    enc, uniq, mapping = encode_labels_for_cmap(labels, include_noise=include_noise, noise_label=noise_label)\n",
    "\n",
    "    base = plt.cm.get_cmap(cmap_name, len(uniq))\n",
    "    colors = [base(i) for i in range(len(uniq))]\n",
    "    cmap = ListedColormap(colors)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(X2[:, 0], X2[:, 1], s=10, alpha=0.8, c=enc, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.grid(True, alpha=0.2)\n",
    "\n",
    "    handles = []\n",
    "    for lab in uniq:\n",
    "        if not (np.asarray(labels).dtype.kind in [\"U\", \"S\", \"O\"]) and lab == noise_label:\n",
    "            name = \"Noise (-1)\"\n",
    "        else:\n",
    "            name = str(lab) if (np.asarray(labels).dtype.kind in [\"U\", \"S\", \"O\"]) else f\"Cluster {int(lab)}\"\n",
    "\n",
    "        handles.append(\n",
    "            Line2D([0], [0], marker=\"o\", linestyle=\"\", markerfacecolor=cmap(mapping[lab]),\n",
    "                   markeredgecolor=\"none\", markersize=6, label=name)\n",
    "        )\n",
    "    plt.legend(handles=handles, title=legend_title, loc=\"best\", frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ensure_dirs(outpath.parent)\n",
    "    plt.savefig(outpath, dpi=200)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_pca2_side_by_side_truth_vs_cluster(X: np.ndarray, y_true: np.ndarray, cluster_labels: np.ndarray,\n",
    "                                           title: str, outpath: Path,\n",
    "                                           include_noise: bool = True, noise_label: int = -1,\n",
    "                                           cmap_true: str = \"Set1\", cmap_cluster: str = \"Set2\",\n",
    "                                           show: bool = True) -> None:\n",
    "    X2 = pca2_project(X)\n",
    "\n",
    "    y_enc, uniq_true, map_true = encode_labels_for_cmap(y_true, include_noise=False, noise_label=noise_label)\n",
    "    c_enc, uniq_cl, map_cl = encode_labels_for_cmap(cluster_labels, include_noise=include_noise, noise_label=noise_label)\n",
    "\n",
    "    cmapT = plt.cm.get_cmap(cmap_true, len(uniq_true))\n",
    "    cmapC = plt.cm.get_cmap(cmap_cluster, len(uniq_cl))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True, sharey=True)\n",
    "\n",
    "    axes[0].scatter(X2[:, 0], X2[:, 1], s=10, alpha=0.8, c=y_enc, cmap=cmapT)\n",
    "    axes[0].set_title(\"TRUE (language)\")\n",
    "    axes[0].grid(True, alpha=0.2)\n",
    "    true_handles = [\n",
    "        Line2D([0], [0], marker=\"o\", linestyle=\"\", markerfacecolor=cmapT(i), markeredgecolor=\"none\",\n",
    "               markersize=6, label=str(lab))\n",
    "        for i, lab in enumerate(uniq_true)\n",
    "    ]\n",
    "    axes[0].legend(handles=true_handles, title=\"Language\", loc=\"best\", frameon=True)\n",
    "\n",
    "    axes[1].scatter(X2[:, 0], X2[:, 1], s=10, alpha=0.8, c=c_enc, cmap=cmapC)\n",
    "    axes[1].set_title(\"CLUSTER\")\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "    cl_handles = []\n",
    "    for i, lab in enumerate(uniq_cl):\n",
    "        if lab == noise_label:\n",
    "            name = \"Noise (-1)\"\n",
    "        else:\n",
    "            name = f\"Cluster {int(lab)}\"\n",
    "        cl_handles.append(\n",
    "            Line2D([0], [0], marker=\"o\", linestyle=\"\", markerfacecolor=cmapC(i), markeredgecolor=\"none\",\n",
    "                   markersize=6, label=name)\n",
    "        )\n",
    "    axes[1].legend(handles=cl_handles, title=\"Clusters\", loc=\"best\", frameon=True)\n",
    "\n",
    "    fig.suptitle(title, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    ensure_dirs(outpath.parent)\n",
    "    fig.savefig(outpath, dpi=200)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c2fbe",
   "metadata": {},
   "source": [
    "## 4. Optional CSV Export for Reporting and Manual Inspection\n",
    "\n",
    "This section exports feature names and (when available) metadata to CSV, and serializes the train/test matrices to tabular form. Such exports are useful for debugging, external reporting, and interoperability with non-Python tooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b60cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step0_export_to_csv() -> Path:\n",
    "    feature_names = load_feature_names(DATA_DIR)\n",
    "\n",
    "    pd.DataFrame({\"feature\": feature_names}).to_csv(EXPORT_DIR / \"feature_names.csv\", index=False)\n",
    "    print(\"[SAVED] feature_names.csv ->\", EXPORT_DIR / \"feature_names.csv\")\n",
    "\n",
    "    md_path = DATA_DIR / \"metadata.csv\"\n",
    "    if md_path.exists():\n",
    "        md = pd.read_csv(md_path)\n",
    "        md.to_csv(EXPORT_DIR / \"metadata.csv\", index=False)\n",
    "        print(\"[SAVED] metadata.csv ->\", EXPORT_DIR / \"metadata.csv\")\n",
    "    else:\n",
    "        print(\"[WARN] metadata.csv پیدا نشد؛ از این بخش رد شدم.\")\n",
    "\n",
    "    for kind in KINDS:\n",
    "        base = DATA_DIR / kind\n",
    "        if not base.exists():\n",
    "            print(f\"[WARN] پوشه داده‌ها برای {kind} وجود ندارد: {base} (skip)\")\n",
    "            continue\n",
    "\n",
    "        X_train, X_test, y_train, y_test = load_dataset_npy(DATA_DIR, kind)\n",
    "\n",
    "        df_X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "        df_X_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "        kind_out = EXPORT_DIR / kind\n",
    "        ensure_dirs(kind_out)\n",
    "\n",
    "        df_X_train.to_csv(kind_out / \"X_train.csv\", index=False)\n",
    "        df_X_test.to_csv(kind_out / \"X_test.csv\", index=False)\n",
    "        pd.DataFrame({\"label\": y_train}).to_csv(kind_out / \"y_train.csv\", index=False)\n",
    "        pd.DataFrame({\"label\": y_test}).to_csv(kind_out / \"y_test.csv\", index=False)\n",
    "\n",
    "        df_train = df_X_train.copy()\n",
    "        df_train[\"label\"] = y_train\n",
    "        df_test = df_X_test.copy()\n",
    "        df_test[\"label\"] = y_test\n",
    "\n",
    "        df_train.to_csv(kind_out / \"train_with_label.csv\", index=False)\n",
    "        df_test.to_csv(kind_out / \"test_with_label.csv\", index=False)\n",
    "\n",
    "        print(f\"[SAVED] CSV export for kind={kind} -> {kind_out}\")\n",
    "\n",
    "    print(\"[DONE] Step0 finished. EXPORT_DIR:\", EXPORT_DIR)\n",
    "    return EXPORT_DIR\n",
    "\n",
    "\n",
    "# Run step0 (uncomment if needed)\n",
    "step0_export_to_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00115dce",
   "metadata": {},
   "source": [
    "## 5. Step 1 — Preprocessing and Feature-Space Optimization\n",
    "\n",
    "This block implements the preprocessing stage that prepares both **raw** and **optimized** feature representations. The optimization may include removing highly correlated features, optional PCA (variance-retaining dimensionality reduction), and optional row-wise normalization; the resulting artifacts are saved for downstream clustering experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397fabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_corr(X_train: np.ndarray, X_test: np.ndarray, feature_names: list[str], thr: float):\n",
    "    corr = np.corrcoef(X_train, rowvar=False)\n",
    "    to_drop = set()\n",
    "\n",
    "    for i in range(corr.shape[0]):\n",
    "        if i in to_drop:\n",
    "            continue\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if abs(corr[i, j]) > thr:\n",
    "                to_drop.add(j)\n",
    "\n",
    "    to_drop = sorted(to_drop)\n",
    "    kept_mask = np.ones(X_train.shape[1], dtype=bool)\n",
    "    kept_mask[to_drop] = False\n",
    "    kept_idx = np.where(kept_mask)[0]\n",
    "\n",
    "    dropped_features = [feature_names[i] for i in to_drop]\n",
    "    kept_features = [feature_names[i] for i in kept_idx]\n",
    "\n",
    "    return (\n",
    "        X_train[:, kept_mask],\n",
    "        X_test[:, kept_mask],\n",
    "        kept_idx,\n",
    "        kept_features,\n",
    "        dropped_features,\n",
    "    )\n",
    "\n",
    "\n",
    "def step1_prepare_raw_and_optimized() -> Path:\n",
    "    out_root = RUN_DIR / \"step1_raw_and_optimized_data\"\n",
    "    ensure_dirs(out_root)\n",
    "\n",
    "    feature_names = load_feature_names(DATA_DIR)\n",
    "    pd.DataFrame({\"feature\": feature_names}).to_csv(out_root / \"feature_names.csv\", index=False)\n",
    "\n",
    "    for kind in KINDS:\n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(\"[KIND]\", kind)\n",
    "\n",
    "        X_train_raw, X_test_raw, y_train, y_test = load_dataset_npy(DATA_DIR, kind)\n",
    "\n",
    "        kind_root = out_root / kind\n",
    "        if kind_root.exists():\n",
    "            shutil.rmtree(kind_root)\n",
    "        ensure_dirs(kind_root)\n",
    "\n",
    "        raw_dir = kind_root / \"raw\"\n",
    "        opt_dir = kind_root / \"optimized\"\n",
    "        meta_dir = kind_root / \"meta\"\n",
    "        ensure_dirs(raw_dir, opt_dir, meta_dir)\n",
    "\n",
    "        # ---- save RAW (csv + npy) ----\n",
    "        pd.DataFrame(X_train_raw, columns=feature_names).to_csv(raw_dir / \"X_train.csv\", index=False)\n",
    "        pd.DataFrame(X_test_raw, columns=feature_names).to_csv(raw_dir / \"X_test.csv\", index=False)\n",
    "        pd.DataFrame({\"label\": y_train}).to_csv(raw_dir / \"y_train.csv\", index=False)\n",
    "        pd.DataFrame({\"label\": y_test}).to_csv(raw_dir / \"y_test.csv\", index=False)\n",
    "\n",
    "        np.save(raw_dir / \"X_train.npy\", X_train_raw)\n",
    "        np.save(raw_dir / \"X_test.npy\", X_test_raw)\n",
    "        np.save(raw_dir / \"y_train.npy\", y_train)\n",
    "        np.save(raw_dir / \"y_test.npy\", y_test)\n",
    "\n",
    "        # ---- build OPT ----\n",
    "        X_train_opt = X_train_raw.copy()\n",
    "        X_test_opt = X_test_raw.copy()\n",
    "        current_feature_names = feature_names[:]\n",
    "\n",
    "        report = {\n",
    "            \"kind\": kind,\n",
    "            \"input_shape_train\": list(X_train_raw.shape),\n",
    "            \"input_shape_test\": list(X_test_raw.shape),\n",
    "            \"ops\": [],\n",
    "        }\n",
    "\n",
    "        if DROP_HIGH_CORR:\n",
    "            X_train_opt, X_test_opt, kept_idx, kept_features, dropped_features = drop_high_corr(\n",
    "                X_train_opt, X_test_opt, current_feature_names, thr=CORR_THR\n",
    "            )\n",
    "            current_feature_names = kept_features\n",
    "\n",
    "            np.save(meta_dir / \"kept_feature_indices.npy\", kept_idx)\n",
    "            pd.DataFrame({\"kept_feature\": kept_features}).to_csv(meta_dir / \"kept_feature_names.csv\", index=False)\n",
    "            pd.DataFrame({\"dropped_feature\": dropped_features}).to_csv(meta_dir / \"dropped_feature_names.csv\", index=False)\n",
    "\n",
    "            report[\"ops\"].append(\n",
    "                {\n",
    "                    \"name\": \"drop_high_corr\",\n",
    "                    \"corr_thr\": CORR_THR,\n",
    "                    \"dropped_count\": int(len(dropped_features)),\n",
    "                    \"kept_count\": int(len(kept_features)),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if USE_PCA:\n",
    "            pca = PCA(n_components=PCA_VAR, whiten=PCA_WHITEN, svd_solver=\"full\")\n",
    "            X_train_opt = pca.fit_transform(X_train_opt)\n",
    "            X_test_opt = pca.transform(X_test_opt)\n",
    "\n",
    "            n_comp = X_train_opt.shape[1]\n",
    "            current_feature_names = [f\"pc_{i+1}\" for i in range(n_comp)]\n",
    "\n",
    "            np.save(meta_dir / \"pca_components.npy\", pca.components_)\n",
    "            np.save(meta_dir / \"pca_explained_variance_ratio.npy\", pca.explained_variance_ratio_)\n",
    "\n",
    "            report[\"ops\"].append(\n",
    "                {\n",
    "                    \"name\": \"pca\",\n",
    "                    \"n_components\": int(n_comp),\n",
    "                    \"pca_var_target\": float(PCA_VAR),\n",
    "                    \"explained_variance_ratio_sum\": float(np.sum(pca.explained_variance_ratio_)),\n",
    "                    \"whiten\": bool(PCA_WHITEN),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if ROW_L2_NORM:\n",
    "            X_train_opt = normalize(X_train_opt, norm=\"l2\", axis=1, copy=True)\n",
    "            X_test_opt = normalize(X_test_opt, norm=\"l2\", axis=1, copy=True)\n",
    "            report[\"ops\"].append({\"name\": \"row_l2_normalize\"})\n",
    "\n",
    "        report[\"output_shape_train\"] = list(X_train_opt.shape)\n",
    "        report[\"output_shape_test\"] = list(X_test_opt.shape)\n",
    "\n",
    "        # ---- save OPT (csv + npy) ----\n",
    "        pd.DataFrame({\"feature\": current_feature_names}).to_csv(opt_dir / \"feature_names_opt.csv\", index=False)\n",
    "\n",
    "        pd.DataFrame(X_train_opt, columns=current_feature_names).to_csv(opt_dir / \"X_train.csv\", index=False)\n",
    "        pd.DataFrame(X_test_opt, columns=current_feature_names).to_csv(opt_dir / \"X_test.csv\", index=False)\n",
    "        pd.DataFrame({\"label\": y_train}).to_csv(opt_dir / \"y_train.csv\", index=False)\n",
    "        pd.DataFrame({\"label\": y_test}).to_csv(opt_dir / \"y_test.csv\", index=False)\n",
    "\n",
    "        np.save(opt_dir / \"X_train.npy\", X_train_opt)\n",
    "        np.save(opt_dir / \"X_test.npy\", X_test_opt)\n",
    "        np.save(opt_dir / \"y_train.npy\", y_train)\n",
    "        np.save(opt_dir / \"y_test.npy\", y_test)\n",
    "\n",
    "        save_json(report, meta_dir / \"opt_pipeline_report.json\")\n",
    "        print(f\"[DONE] {kind} -> RAW:{raw_dir} | OPT:{opt_dir}\")\n",
    "\n",
    "    save_json({\"run_dir\": str(RUN_DIR), \"kinds\": KINDS}, out_root / \"step1_index.json\")\n",
    "    print(\"\\n[ALL DONE] Step1 outputs:\", out_root)\n",
    "    return out_root\n",
    "\n",
    "\n",
    "# Run step1\n",
    "step1_prepare_raw_and_optimized()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b53424",
   "metadata": {},
   "source": [
    "## 6. Step 2 — K-Means: Model Selection and Comparative Evaluation\n",
    "\n",
    "In this section, K-Means is fit over a range of cluster counts and evaluated (e.g., via silhouette score, and agreement-based summaries where labels are available). Results are aggregated and visualized to support a defensible selection of `k` and to compare performance between raw and optimized representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccba2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_step1_variant(kind: str, variant: str):\n",
    "    base = RUN_DIR / \"step1_raw_and_optimized_data\" / kind / variant\n",
    "    X_train = np.load(base / \"X_train.npy\")\n",
    "    y_train = np.load(base / \"y_train.npy\", allow_pickle=True).astype(str)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def make_kmeans(k: int) -> KMeans:\n",
    "    # For compatibility with older sklearn versions (n_init=\"auto\" vs int)\n",
    "    try:\n",
    "        return KMeans(n_clusters=k, n_init=\"auto\", random_state=RANDOM_SEED)\n",
    "    except TypeError:\n",
    "        return KMeans(n_clusters=k, n_init=10, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "def kmeans_sweep(X: np.ndarray, k_list: list[int]):\n",
    "    rows = []\n",
    "    labels_by_k = {}\n",
    "\n",
    "    n = X.shape[0]\n",
    "    sample_size = SIL_SAMPLE_SIZE if n > SIL_SAMPLE_SIZE else None\n",
    "\n",
    "    for k in k_list:\n",
    "        km = make_kmeans(k)\n",
    "        labels = km.fit_predict(X)\n",
    "\n",
    "        inertia = float(km.inertia_)\n",
    "        sil = float(\"nan\") if len(np.unique(labels)) < 2 else float(\n",
    "            silhouette_score(X, labels, sample_size=sample_size, random_state=RANDOM_SEED)\n",
    "        )\n",
    "\n",
    "        rows.append({\"k\": int(k), \"inertia\": inertia, \"silhouette\": sil})\n",
    "        labels_by_k[int(k)] = labels\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df_valid = df.dropna(subset=[\"silhouette\"])\n",
    "    best = df_valid.loc[df_valid[\"silhouette\"].idxmax()].to_dict()\n",
    "    best_k = int(best[\"k\"])\n",
    "    return df, best, labels_by_k[best_k], {\"silhouette_sample_size\": sample_size}\n",
    "\n",
    "\n",
    "def plot_metric_curve(df: pd.DataFrame, ycol: str, title: str, out_png: Path) -> None:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(df[\"k\"], df[ycol], marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(ycol)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    ensure_dirs(out_png.parent)\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_metric_curve_side_by_side(df_raw: pd.DataFrame, df_opt: pd.DataFrame, ycol: str,\n",
    "                                  title: str, out_png: Path) -> None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "    axes[0].plot(df_raw[\"k\"], df_raw[ycol], marker=\"o\")\n",
    "    axes[0].set_title(\"RAW\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(df_opt[\"k\"], df_opt[ycol], marker=\"o\")\n",
    "    axes[1].set_title(\"OPT\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    fig.suptitle(title, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    ensure_dirs(out_png.parent)\n",
    "    fig.savefig(out_png, dpi=200)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def step2_kmeans_compare() -> Path:\n",
    "    out_root = RUN_DIR / \"step2_kmeans_compare\"\n",
    "    ensure_dirs(out_root)\n",
    "\n",
    "    for kind in KINDS:\n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(\"[KIND]\", kind)\n",
    "\n",
    "        raw_out, opt_out, cmp_out = prepare_compare_dirs(out_root, kind)\n",
    "\n",
    "        Xr, yr = load_step1_variant(kind, \"raw\")\n",
    "        Xo, yo = load_step1_variant(kind, \"optimized\")\n",
    "\n",
    "        df_raw, best_raw, best_labels_raw, raw_meta = kmeans_sweep(Xr, K_RANGE)\n",
    "        df_opt, best_opt, best_labels_opt, opt_meta = kmeans_sweep(Xo, K_RANGE)\n",
    "\n",
    "        df_raw.to_csv(raw_out / \"tables\" / \"kmeans_metrics.csv\", index=False)\n",
    "        df_opt.to_csv(opt_out / \"tables\" / \"kmeans_metrics.csv\", index=False)\n",
    "\n",
    "        pd.DataFrame({\"cluster\": best_labels_raw}).to_csv(raw_out / \"tables\" / \"labels_best.csv\", index=False)\n",
    "        pd.DataFrame({\"cluster\": best_labels_opt}).to_csv(opt_out / \"tables\" / \"labels_best.csv\", index=False)\n",
    "\n",
    "        pur_raw, pur_table_raw = purity_score(best_labels_raw, yr, noise_label=None)\n",
    "        pur_opt, pur_table_opt = purity_score(best_labels_opt, yo, noise_label=None)\n",
    "\n",
    "        pur_table_raw.to_csv(raw_out / \"tables\" / \"purity_best.csv\", index=False)\n",
    "        pur_table_opt.to_csv(opt_out / \"tables\" / \"purity_best.csv\", index=False)\n",
    "\n",
    "        save_json({\"best\": best_raw, **raw_meta, \"purity_overall\": pur_raw}, raw_out / \"meta\" / \"summary.json\")\n",
    "        save_json({\"best\": best_opt, **opt_meta, \"purity_overall\": pur_opt}, opt_out / \"meta\" / \"summary.json\")\n",
    "\n",
    "        print(f\"[BEST] RAW k={int(best_raw['k'])} sil={best_raw['silhouette']:.4f} purity={pur_raw:.4f}\")\n",
    "        print(f\"[BEST] OPT k={int(best_opt['k'])} sil={best_opt['silhouette']:.4f} purity={pur_opt:.4f}\")\n",
    "\n",
    "        side_by_side_tables(df_raw, df_opt, title_left=f\"RAW ({kind})\", title_right=f\"OPT ({kind})\")\n",
    "\n",
    "        # plots\n",
    "        plot_metric_curve(df_raw, \"inertia\", f\"Elbow (Inertia) — RAW — {kind}\", raw_out / \"figures\" / \"elbow_inertia.png\")\n",
    "        plot_metric_curve(df_opt, \"inertia\", f\"Elbow (Inertia) — OPT — {kind}\", opt_out / \"figures\" / \"elbow_inertia.png\")\n",
    "        plot_metric_curve(df_raw, \"silhouette\", f\"Silhouette vs k — RAW — {kind}\", raw_out / \"figures\" / \"silhouette.png\")\n",
    "        plot_metric_curve(df_opt, \"silhouette\", f\"Silhouette vs k — OPT — {kind}\", opt_out / \"figures\" / \"silhouette.png\")\n",
    "\n",
    "        plot_metric_curve_side_by_side(df_raw, df_opt, \"inertia\",\n",
    "                                       f\"Elbow (Inertia) side-by-side — {kind}\",\n",
    "                                       cmp_out / \"figures\" / \"elbow_side_by_side.png\")\n",
    "        plot_metric_curve_side_by_side(df_raw, df_opt, \"silhouette\",\n",
    "                                       f\"Silhouette side-by-side — {kind}\",\n",
    "                                       cmp_out / \"figures\" / \"silhouette_side_by_side.png\")\n",
    "\n",
    "        # PCA2 visuals (true + best clusters)\n",
    "        plot_pca2_scatter(Xr, yr, f\"PCA2 TRUE labels — RAW — {kind}\",\n",
    "                          raw_out / \"figures\" / \"pca2_true_labels.png\",\n",
    "                          legend_title=\"Language\", include_noise=False)\n",
    "        plot_pca2_scatter(Xo, yo, f\"PCA2 TRUE labels — OPT — {kind}\",\n",
    "                          opt_out / \"figures\" / \"pca2_true_labels.png\",\n",
    "                          legend_title=\"Language\", include_noise=False)\n",
    "\n",
    "        plot_pca2_scatter(Xr, best_labels_raw, f\"PCA2 KMeans best — RAW — {kind} (k={int(best_raw['k'])})\",\n",
    "                          raw_out / \"figures\" / \"pca2_kmeans_best.png\",\n",
    "                          legend_title=\"Clusters\", include_noise=False)\n",
    "        plot_pca2_scatter(Xo, best_labels_opt, f\"PCA2 KMeans best — OPT — {kind} (k={int(best_opt['k'])})\",\n",
    "                          opt_out / \"figures\" / \"pca2_kmeans_best.png\",\n",
    "                          legend_title=\"Clusters\", include_noise=False)\n",
    "\n",
    "        plot_pca2_side_by_side_truth_vs_cluster(\n",
    "            Xr, yr, best_labels_raw,\n",
    "            title=f\"RAW — {kind} — TRUE vs KMeans (k={int(best_raw['k'])})\",\n",
    "            outpath=cmp_out / \"figures\" / \"raw_truth_vs_kmeans.png\",\n",
    "            include_noise=False,\n",
    "        )\n",
    "        plot_pca2_side_by_side_truth_vs_cluster(\n",
    "            Xo, yo, best_labels_opt,\n",
    "            title=f\"OPT — {kind} — TRUE vs KMeans (k={int(best_opt['k'])})\",\n",
    "            outpath=cmp_out / \"figures\" / \"opt_truth_vs_kmeans.png\",\n",
    "            include_noise=False,\n",
    "        )\n",
    "\n",
    "        # combined metrics table\n",
    "        df_raw2 = df_raw.assign(variant=\"raw\")\n",
    "        df_opt2 = df_opt.assign(variant=\"optimized\")\n",
    "        pd.concat([df_raw2, df_opt2], ignore_index=True).to_csv(cmp_out / \"tables\" / \"kmeans_metrics_combined.csv\", index=False)\n",
    "\n",
    "    print(\"\\n[DONE] Step2 completed. Saved under:\", out_root)\n",
    "    return out_root\n",
    "\n",
    "\n",
    "# Run step2\n",
    "step2_kmeans_compare()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ba4b4",
   "metadata": {},
   "source": [
    "## 7. Step 3 — DBSCAN: Density-Based Clustering and ε (Epsilon) Selection\n",
    "\n",
    "This block evaluates DBSCAN under multiple `min_samples` settings and candidate `ε` values derived from the *k-distance* curve. Because DBSCAN can assign **noise points**, evaluation is performed with noise-aware metrics and constraints (e.g., maximum admissible noise ratio), enabling principled selection of feasible configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_distance_curve(X: np.ndarray, k: int) -> np.ndarray:\n",
    "    nn = NearestNeighbors(n_neighbors=k)\n",
    "    nn.fit(X)\n",
    "    dists, _ = nn.kneighbors(X)\n",
    "    kth = dists[:, -1]\n",
    "    return np.sort(kth)\n",
    "\n",
    "\n",
    "def eps_candidates_from_percentiles(kth_sorted: np.ndarray, percentiles: list[int]) -> list[float]:\n",
    "    return [float(np.percentile(kth_sorted, p)) for p in percentiles]\n",
    "\n",
    "\n",
    "def dbscan_eval(X: np.ndarray, y_true: np.ndarray, eps: float, min_samples: int):\n",
    "    model = DBSCAN(eps=float(eps), min_samples=int(min_samples))\n",
    "    labels = model.fit_predict(X)\n",
    "\n",
    "    n = len(labels)\n",
    "    n_noise = int(np.sum(labels == -1))\n",
    "    noise_ratio = float(n_noise / n)\n",
    "\n",
    "    clusters = [c for c in np.unique(labels) if c != -1]\n",
    "    n_clusters = int(len(clusters))\n",
    "\n",
    "    sil = silhouette_excluding_noise(X, labels, noise_label=-1, sample_size=SIL_SAMPLE_SIZE)\n",
    "    pur, pur_tbl = purity_score(labels, y_true, noise_label=-1)\n",
    "\n",
    "    sizes = pd.Series(labels[labels != -1]).value_counts().sort_index()\n",
    "    min_size = int(sizes.min()) if len(sizes) else 0\n",
    "    max_size = int(sizes.max()) if len(sizes) else 0\n",
    "\n",
    "    row = {\n",
    "        \"eps\": float(eps),\n",
    "        \"min_samples\": int(min_samples),\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"noise_ratio\": noise_ratio,\n",
    "        \"silhouette_excl_noise\": sil,\n",
    "        \"purity_excl_noise\": float(pur),\n",
    "        \"min_cluster_size\": min_size,\n",
    "        \"max_cluster_size\": max_size,\n",
    "    }\n",
    "    return labels, row, pur_tbl\n",
    "\n",
    "\n",
    "def step3_dbscan_compare() -> Path:\n",
    "    out_root = RUN_DIR / \"step3_dbscan_compare\"\n",
    "    ensure_dirs(out_root)\n",
    "\n",
    "    for kind in KINDS:\n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(\"[KIND]\", kind)\n",
    "\n",
    "        raw_out, opt_out, cmp_out = prepare_compare_dirs(out_root, kind)\n",
    "\n",
    "        Xr, yr = load_step1_variant(kind, \"raw\")\n",
    "        Xo, yo = load_step1_variant(kind, \"optimized\")\n",
    "\n",
    "        # ---- RAW grid ----\n",
    "        raw_rows = []\n",
    "        raw_best = None\n",
    "        raw_best_labels = None\n",
    "        raw_best_purity_tbl = None\n",
    "\n",
    "        for ms in DBSCAN_MIN_SAMPLES_LIST:\n",
    "            kth_sorted = k_distance_curve(Xr, k=ms)\n",
    "            eps_list = eps_candidates_from_percentiles(kth_sorted, DBSCAN_EPS_PERCENTILES)\n",
    "\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(kth_sorted)\n",
    "            plt.title(f\"k-distance curve (RAW) — {kind} — min_samples={ms}\")\n",
    "            plt.xlabel(\"points sorted\")\n",
    "            plt.ylabel(f\"distance to {ms}-th NN\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(raw_out / \"figures\" / f\"kdist_ms_{ms}.png\", dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "            for eps in eps_list:\n",
    "                labels, row, pur_tbl = dbscan_eval(Xr, yr, eps=eps, min_samples=ms)\n",
    "                raw_rows.append(row)\n",
    "\n",
    "        df_raw = pd.DataFrame(raw_rows).sort_values([\"silhouette_excl_noise\", \"noise_ratio\"], ascending=[False, True])\n",
    "        df_raw.to_csv(raw_out / \"tables\" / \"dbscan_grid_results.csv\", index=False)\n",
    "\n",
    "        raw_best = choose_best_by_target_k(df_raw, target_k=TARGET_K, max_noise_ratio=DBSCAN_MAX_NOISE_RATIO)\n",
    "        if raw_best is not None:\n",
    "            raw_best_labels, _, raw_best_purity_tbl = dbscan_eval(\n",
    "                Xr, yr, eps=raw_best[\"eps\"], min_samples=int(raw_best[\"min_samples\"])\n",
    "            )\n",
    "            pd.DataFrame({\"cluster\": raw_best_labels}).to_csv(raw_out / \"tables\" / \"labels_best.csv\", index=False)\n",
    "            raw_best_purity_tbl.to_csv(raw_out / \"tables\" / \"purity_best.csv\", index=False)\n",
    "            save_json(raw_best, raw_out / \"meta\" / \"best_params.json\")\n",
    "\n",
    "            plot_pca2_scatter(\n",
    "                Xr, raw_best_labels,\n",
    "                title=f\"DBSCAN BEST (RAW) — {kind} — eps={raw_best['eps']:.4f}, ms={int(raw_best['min_samples'])}\",\n",
    "                outpath=raw_out / \"figures\" / \"pca2_best_clusters.png\",\n",
    "                include_noise=True,\n",
    "            )\n",
    "            plot_pca2_side_by_side_truth_vs_cluster(\n",
    "                Xr, yr, raw_best_labels,\n",
    "                title=f\"RAW — {kind} — TRUE vs DBSCAN (eps={raw_best['eps']:.4f}, ms={int(raw_best['min_samples'])})\",\n",
    "                outpath=cmp_out / \"figures\" / \"raw_truth_vs_dbscan.png\",\n",
    "                include_noise=True,\n",
    "            )\n",
    "\n",
    "        # ---- OPT grid ----\n",
    "        opt_rows = []\n",
    "        opt_best = None\n",
    "        opt_best_labels = None\n",
    "        opt_best_purity_tbl = None\n",
    "\n",
    "        for ms in DBSCAN_MIN_SAMPLES_LIST:\n",
    "            kth_sorted = k_distance_curve(Xo, k=ms)\n",
    "            eps_list = eps_candidates_from_percentiles(kth_sorted, DBSCAN_EPS_PERCENTILES)\n",
    "\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(kth_sorted)\n",
    "            plt.title(f\"k-distance curve (OPT) — {kind} — min_samples={ms}\")\n",
    "            plt.xlabel(\"points sorted\")\n",
    "            plt.ylabel(f\"distance to {ms}-th NN\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(opt_out / \"figures\" / f\"kdist_ms_{ms}.png\", dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "            for eps in eps_list:\n",
    "                labels, row, pur_tbl = dbscan_eval(Xo, yo, eps=eps, min_samples=ms)\n",
    "                opt_rows.append(row)\n",
    "\n",
    "        df_opt = pd.DataFrame(opt_rows).sort_values([\"silhouette_excl_noise\", \"noise_ratio\"], ascending=[False, True])\n",
    "        df_opt.to_csv(opt_out / \"tables\" / \"dbscan_grid_results.csv\", index=False)\n",
    "\n",
    "        opt_best = choose_best_by_target_k(df_opt, target_k=TARGET_K, max_noise_ratio=DBSCAN_MAX_NOISE_RATIO)\n",
    "        if opt_best is not None:\n",
    "            opt_best_labels, _, opt_best_purity_tbl = dbscan_eval(\n",
    "                Xo, yo, eps=opt_best[\"eps\"], min_samples=int(opt_best[\"min_samples\"])\n",
    "            )\n",
    "            pd.DataFrame({\"cluster\": opt_best_labels}).to_csv(opt_out / \"tables\" / \"labels_best.csv\", index=False)\n",
    "            opt_best_purity_tbl.to_csv(opt_out / \"tables\" / \"purity_best.csv\", index=False)\n",
    "            save_json(opt_best, opt_out / \"meta\" / \"best_params.json\")\n",
    "\n",
    "            plot_pca2_scatter(\n",
    "                Xo, opt_best_labels,\n",
    "                title=f\"DBSCAN BEST (OPT) — {kind} — eps={opt_best['eps']:.4f}, ms={int(opt_best['min_samples'])}\",\n",
    "                outpath=opt_out / \"figures\" / \"pca2_best_clusters.png\",\n",
    "                include_noise=True,\n",
    "            )\n",
    "            plot_pca2_side_by_side_truth_vs_cluster(\n",
    "                Xo, yo, opt_best_labels,\n",
    "                title=f\"OPT — {kind} — TRUE vs DBSCAN (eps={opt_best['eps']:.4f}, ms={int(opt_best['min_samples'])})\",\n",
    "                outpath=cmp_out / \"figures\" / \"opt_truth_vs_dbscan.png\",\n",
    "                include_noise=True,\n",
    "            )\n",
    "\n",
    "        # ---- Show top-10 tables ----\n",
    "        top_raw = df_raw.head(10).reset_index(drop=True)\n",
    "        top_opt = df_opt.head(10).reset_index(drop=True)\n",
    "        print(\"[BEST RAW]\", raw_best)\n",
    "        print(\"[BEST OPT]\", opt_best)\n",
    "        side_by_side_tables(top_raw, top_opt, title_left=f\"RAW Top-10 ({kind})\", title_right=f\"OPT Top-10 ({kind})\")\n",
    "\n",
    "        # ---- one combined figure: k-distance + PCA2 best ----\n",
    "        ms_show = 10 if 10 in DBSCAN_MIN_SAMPLES_LIST else DBSCAN_MIN_SAMPLES_LIST[0]\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "        rk = k_distance_curve(Xr, k=ms_show)\n",
    "        ok = k_distance_curve(Xo, k=ms_show)\n",
    "\n",
    "        axes[0, 0].plot(rk)\n",
    "        axes[0, 0].set_title(f\"RAW k-distance (ms={ms_show}) — {kind}\")\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[0, 1].plot(ok)\n",
    "        axes[0, 1].set_title(f\"OPT k-distance (ms={ms_show}) — {kind}\")\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        if raw_best_labels is not None:\n",
    "            X2 = pca2_project(Xr)\n",
    "            axes[1, 0].scatter(X2[:, 0], X2[:, 1], s=10, alpha=0.8, c=raw_best_labels)\n",
    "            axes[1, 0].set_title(\"RAW best clusters (PCA2 view)\")\n",
    "            axes[1, 0].grid(True, alpha=0.2)\n",
    "        else:\n",
    "            axes[1, 0].set_title(\"RAW best clusters (not found)\")\n",
    "            axes[1, 0].axis(\"off\")\n",
    "\n",
    "        if opt_best_labels is not None:\n",
    "            X2 = pca2_project(Xo)\n",
    "            axes[1, 1].scatter(X2[:, 0], X2[:, 1], s=10, alpha=0.8, c=opt_best_labels)\n",
    "            axes[1, 1].set_title(\"OPT best clusters (PCA2 view)\")\n",
    "            axes[1, 1].grid(True, alpha=0.2)\n",
    "        else:\n",
    "            axes[1, 1].set_title(\"OPT best clusters (not found)\")\n",
    "            axes[1, 1].axis(\"off\")\n",
    "\n",
    "        fig.suptitle(f\"DBSCAN side-by-side — {kind}\", y=1.02)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(cmp_out / \"figures\" / \"dbscan_side_by_side.png\", dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # ---- save compare summary table ----\n",
    "        comp = []\n",
    "        if raw_best is not None:\n",
    "            comp.append({\"variant\": \"raw\", **raw_best})\n",
    "        if opt_best is not None:\n",
    "            comp.append({\"variant\": \"optimized\", **opt_best})\n",
    "        pd.DataFrame(comp).to_csv(cmp_out / \"tables\" / \"best_compare.csv\", index=False)\n",
    "\n",
    "    print(\"\\n[DONE] Step3 (DBSCAN) completed. Saved under:\", out_root)\n",
    "    return out_root\n",
    "\n",
    "\n",
    "# Run step3\n",
    "step3_dbscan_compare()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2b592",
   "metadata": {},
   "source": [
    "## 8. Step 4 — OPTICS: Reachability Analysis and Noise-Aware Evaluation\n",
    "\n",
    "Here, OPTICS is used as an alternative density-based method that can recover clusters across varying densities. The workflow fits OPTICS over a grid of parameters (e.g., `min_samples`, `xi`, and `min_cluster_size`), produces reachability diagnostics, and evaluates clusterings while accounting for noise assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8705eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optics_fit_predict(X: np.ndarray, min_samples: int, xi: float, min_cluster_size: float):\n",
    "    model = OPTICS(\n",
    "        min_samples=int(min_samples),\n",
    "        xi=float(xi),\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        cluster_method=\"xi\",\n",
    "    )\n",
    "    labels = model.fit_predict(X)\n",
    "    return model, labels\n",
    "\n",
    "\n",
    "def eval_noise_clustering(X: np.ndarray, y_true: np.ndarray, labels: np.ndarray):\n",
    "    n = len(labels)\n",
    "    noise_ratio = float(np.mean(labels == -1))\n",
    "    n_clusters = int(len([c for c in np.unique(labels) if c != -1]))\n",
    "\n",
    "    sil = silhouette_excluding_noise(X, labels, noise_label=-1, sample_size=SIL_SAMPLE_SIZE)\n",
    "    pur, pur_tbl = purity_score(labels, y_true, noise_label=-1)\n",
    "\n",
    "    return {\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"noise_ratio\": noise_ratio,\n",
    "        \"silhouette_excl_noise\": sil,\n",
    "        \"purity_excl_noise\": float(pur),\n",
    "    }, pur_tbl\n",
    "\n",
    "\n",
    "def plot_reachability(model: OPTICS, title: str, outpath: Path, show: bool = False) -> None:\n",
    "    reachability = model.reachability_[model.ordering_]\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(reachability)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Ordered points\")\n",
    "    plt.ylabel(\"Reachability distance\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    ensure_dirs(outpath.parent)\n",
    "    plt.savefig(outpath, dpi=200)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def step4_optics_compare() -> Path:\n",
    "    out_root = RUN_DIR / \"step4_optics_compare\"\n",
    "    ensure_dirs(out_root)\n",
    "\n",
    "    for kind in KINDS:\n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(\"[KIND]\", kind)\n",
    "\n",
    "        raw_out, opt_out, cmp_out = prepare_compare_dirs(out_root, kind)\n",
    "\n",
    "        Xr, yr = load_step1_variant(kind, \"raw\")\n",
    "        Xo, yo = load_step1_variant(kind, \"optimized\")\n",
    "\n",
    "        # -------- GRID SEARCH RAW --------\n",
    "        raw_rows = []\n",
    "        best_raw = None\n",
    "        best_raw_labels = None\n",
    "        best_raw_model = None\n",
    "        pur_tbl_raw_best = None\n",
    "\n",
    "        for ms in OPTICS_MIN_SAMPLES_LIST:\n",
    "            for xi in OPTICS_XI_LIST:\n",
    "                for mcs in OPTICS_MIN_CLUSTER_SIZE_LIST:\n",
    "                    model, labels = optics_fit_predict(Xr, ms, xi, mcs)\n",
    "                    metrics, _ = eval_noise_clustering(Xr, yr, labels)\n",
    "                    raw_rows.append({\"min_samples\": int(ms), \"xi\": float(xi), \"min_cluster_size\": float(mcs), **metrics})\n",
    "\n",
    "        df_raw = pd.DataFrame(raw_rows).sort_values([\"silhouette_excl_noise\", \"noise_ratio\"], ascending=[False, True])\n",
    "        df_raw.to_csv(raw_out / \"tables\" / \"optics_grid_results.csv\", index=False)\n",
    "\n",
    "        best_raw = choose_best_by_target_k(df_raw, target_k=TARGET_K, max_noise_ratio=OPTICS_MAX_NOISE_RATIO)\n",
    "        if best_raw is not None:\n",
    "            best_raw_model, best_raw_labels = optics_fit_predict(\n",
    "                Xr, best_raw[\"min_samples\"], best_raw[\"xi\"], best_raw[\"min_cluster_size\"]\n",
    "            )\n",
    "            best_metrics_raw, pur_tbl_raw_best = eval_noise_clustering(Xr, yr, best_raw_labels)\n",
    "\n",
    "            save_json({**best_raw, **best_metrics_raw}, raw_out / \"meta\" / \"best_params.json\")\n",
    "            pd.DataFrame({\"cluster\": best_raw_labels}).to_csv(raw_out / \"tables\" / \"labels_best.csv\", index=False)\n",
    "            pur_tbl_raw_best.to_csv(raw_out / \"tables\" / \"purity_best.csv\", index=False)\n",
    "\n",
    "            plot_reachability(\n",
    "                best_raw_model,\n",
    "                title=f\"OPTICS Reachability — RAW — {kind} (ms={best_raw['min_samples']}, xi={best_raw['xi']}, mcs={best_raw['min_cluster_size']})\",\n",
    "                outpath=raw_out / \"figures\" / \"reachability_best.png\",\n",
    "            )\n",
    "            plot_pca2_side_by_side_truth_vs_cluster(\n",
    "                Xr, yr, best_raw_labels,\n",
    "                title=f\"RAW — {kind} — TRUE vs OPTICS\",\n",
    "                outpath=cmp_out / \"figures\" / \"raw_truth_vs_optics.png\",\n",
    "                include_noise=True,\n",
    "            )\n",
    "\n",
    "        # -------- GRID SEARCH OPT --------\n",
    "        opt_rows = []\n",
    "        best_opt = None\n",
    "        best_opt_labels = None\n",
    "        best_opt_model = None\n",
    "        pur_tbl_opt_best = None\n",
    "\n",
    "        for ms in OPTICS_MIN_SAMPLES_LIST:\n",
    "            for xi in OPTICS_XI_LIST:\n",
    "                for mcs in OPTICS_MIN_CLUSTER_SIZE_LIST:\n",
    "                    model, labels = optics_fit_predict(Xo, ms, xi, mcs)\n",
    "                    metrics, _ = eval_noise_clustering(Xo, yo, labels)\n",
    "                    opt_rows.append({\"min_samples\": int(ms), \"xi\": float(xi), \"min_cluster_size\": float(mcs), **metrics})\n",
    "\n",
    "        df_opt = pd.DataFrame(opt_rows).sort_values([\"silhouette_excl_noise\", \"noise_ratio\"], ascending=[False, True])\n",
    "        df_opt.to_csv(opt_out / \"tables\" / \"optics_grid_results.csv\", index=False)\n",
    "\n",
    "        best_opt = choose_best_by_target_k(df_opt, target_k=TARGET_K, max_noise_ratio=OPTICS_MAX_NOISE_RATIO)\n",
    "        if best_opt is not None:\n",
    "            best_opt_model, best_opt_labels = optics_fit_predict(\n",
    "                Xo, best_opt[\"min_samples\"], best_opt[\"xi\"], best_opt[\"min_cluster_size\"]\n",
    "            )\n",
    "            best_metrics_opt, pur_tbl_opt_best = eval_noise_clustering(Xo, yo, best_opt_labels)\n",
    "\n",
    "            save_json({**best_opt, **best_metrics_opt}, opt_out / \"meta\" / \"best_params.json\")\n",
    "            pd.DataFrame({\"cluster\": best_opt_labels}).to_csv(opt_out / \"tables\" / \"labels_best.csv\", index=False)\n",
    "            pur_tbl_opt_best.to_csv(opt_out / \"tables\" / \"purity_best.csv\", index=False)\n",
    "\n",
    "            plot_reachability(\n",
    "                best_opt_model,\n",
    "                title=f\"OPTICS Reachability — OPT — {kind} (ms={best_opt['min_samples']}, xi={best_opt['xi']}, mcs={best_opt['min_cluster_size']})\",\n",
    "                outpath=opt_out / \"figures\" / \"reachability_best.png\",\n",
    "            )\n",
    "            plot_pca2_side_by_side_truth_vs_cluster(\n",
    "                Xo, yo, best_opt_labels,\n",
    "                title=f\"OPT — {kind} — TRUE vs OPTICS\",\n",
    "                outpath=cmp_out / \"figures\" / \"opt_truth_vs_optics.png\",\n",
    "                include_noise=True,\n",
    "            )\n",
    "\n",
    "        # -------- SIDE-BY-SIDE tables --------\n",
    "        top_raw = df_raw.head(10).reset_index(drop=True)\n",
    "        top_opt = df_opt.head(10).reset_index(drop=True)\n",
    "        print(\"[BEST RAW]\", best_raw)\n",
    "        print(\"[BEST OPT]\", best_opt)\n",
    "        side_by_side_tables(top_raw, top_opt, title_left=f\"RAW Top-10 ({kind})\", title_right=f\"OPT Top-10 ({kind})\")\n",
    "\n",
    "        # -------- Side-by-side reachability in one figure --------\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 3), sharey=True)\n",
    "        if best_raw_model is not None:\n",
    "            r = best_raw_model.reachability_[best_raw_model.ordering_]\n",
    "            axes[0].plot(r)\n",
    "            axes[0].set_title(\"RAW reachability\")\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0].set_title(\"RAW reachability (no best)\")\n",
    "            axes[0].axis(\"off\")\n",
    "\n",
    "        if best_opt_model is not None:\n",
    "            r = best_opt_model.reachability_[best_opt_model.ordering_]\n",
    "            axes[1].plot(r)\n",
    "            axes[1].set_title(\"OPT reachability\")\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1].set_title(\"OPT reachability (no best)\")\n",
    "            axes[1].axis(\"off\")\n",
    "\n",
    "        fig.suptitle(f\"OPTICS Reachability side-by-side — {kind}\", y=1.05)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(cmp_out / \"figures\" / \"reachability_side_by_side.png\", dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Save compare summary\n",
    "        comp = []\n",
    "        if best_raw is not None:\n",
    "            comp.append({\"variant\": \"raw\", **best_raw})\n",
    "        if best_opt is not None:\n",
    "            comp.append({\"variant\": \"optimized\", **best_opt})\n",
    "        pd.DataFrame(comp).to_csv(cmp_out / \"tables\" / \"best_compare.csv\", index=False)\n",
    "\n",
    "    print(\"\\n[DONE] Step4 (OPTICS) completed. Saved under:\", out_root)\n",
    "    return out_root\n",
    "\n",
    "\n",
    "# Run step4\n",
    "step4_optics_compare()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452fae64",
   "metadata": {},
   "source": [
    "## 9. Orchestrating the Full Pipeline\n",
    "\n",
    "Finally, a single driver function executes the pipeline end-to-end: exporting (optional), preprocessing, and running the comparative experiments for K-Means, DBSCAN, and OPTICS. This entry-point standardizes execution and ensures that all outputs are produced under a timestamped run directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae753b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    step0_export_to_csv()  # optional\n",
    "    step1_prepare_raw_and_optimized()\n",
    "    step2_kmeans_compare()\n",
    "    step3_dbscan_compare()\n",
    "    step4_optics_compare()\n",
    "\n",
    "run_all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
